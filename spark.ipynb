{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RR_BD",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://apache.volia.net/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!tar -xvf spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1b8k_OVf2QF"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Uz1NL4gHFx"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext('local')\n",
        "spark = SparkSession(sc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAISFqHXf7dt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bf1360e-a111-45c6-9d9b-aecb2dc8f913"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "logData = sc.textFile(\"/content/FW_PG_Meteo_2018-2019.txt\").cache()\n",
        "\n",
        "header = logData.first()\n",
        "\n",
        "logData = logData.filter(lambda x: x != header)\n",
        "\n",
        "logData = logData.map(lambda x: x.split(\", \"))\n",
        "df = logData.toDF(header.split(\",\"))\n",
        "\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+---+----+------+------+----------+----------+--------------+-----------+-----------------+-----------+-------------+---------+-----------------+-----------------+\n",
            "|Year|Month|Day|Hour|Minute|Second|     FW_PG|Wind_Speed|Wind_Direction|Temperature|Relative_Humidity|   Pressure|Precipitation|Dew Point|Absolute_Humidity|Specific_Humidity|\n",
            "+----+-----+---+----+------+------+----------+----------+--------------+-----------+-----------------+-----------+-------------+---------+-----------------+-----------------+\n",
            "|2018|    1|  1|   0|     0|     0|106.772863|  0.240000|      8.930000|   2.690000|        89.670000|1011.000000|     0.000000| 1.149374|         5.223580|         0.804220|\n",
            "|2018|    1|  1|   0|     1|     0|105.187933|  0.290000|      8.900000|   2.740000|        90.250000|1011.010000|     0.000000| 1.288370|         5.275076|         0.805798|\n",
            "|2018|    1|  1|   0|     2|     0|119.689349|  0.100000|      8.110000|   2.600000|        90.110000|1011.000000|     0.000000| 1.128684|         5.217515|         0.803984|\n",
            "|2018|    1|  1|   0|     3|     0|106.389584|  0.080000|      2.920000|   2.520000|        90.350000|1010.990000|     0.000000| 1.086740|         5.203304|         0.803505|\n",
            "|2018|    1|  1|   0|     4|     0|106.443489|  0.080000|    349.300000|   2.570000|        90.750000|1010.980000|     0.000000| 1.197495|         5.243970|         0.804768|\n",
            "|2018|    1|  1|   0|     5|     0|113.447731|  0.080000|    343.190000|   2.510000|        90.720000|1010.950000|     0.000000| 1.133674|         5.221094|         0.804041|\n",
            "|2018|    1|  1|   0|     6|     0|107.098889|  0.080000|    343.380000|   2.510000|        90.920000|1010.920000|     0.000000| 1.164292|         5.232604|         0.804390|\n",
            "|2018|    1|  1|   0|     7|     0|123.023519|  0.080000|    343.730000|   2.390000|        90.890000|1010.920000|     0.000000| 1.041234|         5.188737|         0.802983|\n",
            "|2018|    1|  1|   0|     8|     0|129.590074|  0.080000|    344.540000|   2.310000|        90.960000|1010.930000|     0.000000| 0.972942|         5.164784|         0.802199|\n",
            "|2018|    1|  1|   0|     9|     0|123.573033|  0.080000|    344.500000|   2.300000|        91.160000|1010.940000|     0.000000| 0.993563|         5.172648|         0.802436|\n",
            "|2018|    1|  1|   0|    10|     0|122.207324|  0.080000|    344.500000|   2.250000|        91.260000|1010.960000|     0.000000| 0.959403|         5.160874|         0.802043|\n",
            "|2018|    1|  1|   0|    11|     0|120.075139|  0.080000|    344.510000|   2.250000|        91.460000|1010.960000|     0.000000| 0.989795|         5.172184|         0.802393|\n",
            "|2018|    1|  1|   0|    12|     0|133.212017|  0.080000|    337.660000|   2.300000|        91.700000|1010.980000|     0.000000| 1.075601|         5.203289|         0.803377|\n",
            "|2018|    1|  1|   0|    13|     0|150.234839|  0.080000|    339.380000|   2.280000|        91.760000|1010.990000|     0.000000| 1.064924|         5.199670|         0.803255|\n",
            "|2018|    1|  1|   0|    14|     0|133.270024|  0.080000|    338.520000|   2.310000|        91.950000|1011.000000|     0.000000| 1.123323|         5.220997|         0.803923|\n",
            "|2018|    1|  1|   0|    15|     0|139.346243|  0.080000|    338.060000|   2.310000|        92.000000|1011.010000|     0.000000| 1.130880|         5.223836|         0.804009|\n",
            "|2018|    1|  1|   0|    16|     0|121.012035|  0.080000|    338.080000|   2.320000|        92.180000|1011.020000|     0.000000| 1.167943|         5.237590|         0.804431|\n",
            "|2018|    1|  1|   0|    17|     0|115.757116|  0.080000|    338.080000|   2.380000|        92.400000|1011.020000|     0.000000| 1.260443|         5.271384|         0.805482|\n",
            "|2018|    1|  1|   0|    18|     0|113.019168|  0.080000|    338.070000|   2.440000|        92.560000|1011.030000|     0.000000| 1.343880|         5.301919|         0.806426|\n",
            "|2018|    1|  1|   0|    19|     0|102.077085|  0.080000|    338.080000|   2.490000|        92.720000|1011.040000|     0.000000| 1.417410|         5.329011|         0.807254|\n",
            "+----+-----+---+----+------+------+----------+----------+--------------+-----------+-----------------+-----------+-------------+---------+-----------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pASQ5JX4Hpkx"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Dq0RHawtCvU"
      },
      "source": [
        "\n",
        "for col in df.columns:\n",
        "  df = df.withColumn(col, df[col].cast(DoubleType()))\n",
        "\n",
        "\n",
        "def drop_nan(df):\n",
        "    for col in df.columns:\n",
        "        df = df.na.drop(subset=col)\n",
        "    return df\n",
        "\n",
        "df_d = drop_nan(df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-2WjJCiR8VJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a4eed18-2949-4e36-e60e-a97487867741"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.mllib.util import MLUtils\n",
        "\n",
        "\n",
        "featureIndexer = VectorAssembler(inputCols=['Year','Month', 'Day', 'Hour', 'Minute', 'Wind_Speed', 'Wind_Direction', 'Temperature', 'Relative_Humidity', 'Pressure', 'Dew Point', 'Absolute_Humidity', 'Specific_Humidity'], outputCol = 'indexedFeatures')\n",
        "(trainingData, testData) = df_d.randomSplit([0.7, 0.3])\n",
        "dt = DecisionTreeRegressor(featuresCol=\"indexedFeatures\", labelCol=\"FW_PG\", maxDepth=25, minInstancesPerNode=5)\n",
        "pipeline = Pipeline(stages=[featureIndexer, dt])\n",
        "\n",
        "# Train model.  This also runs the indexer.\n",
        "model = pipeline.fit(trainingData)\n",
        "\n",
        "# Make predictions.\n",
        "predictions_test = model.transform(testData)\n",
        "predictions_train = model.transform(trainingData)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator_mse = RegressionEvaluator(\n",
        "    labelCol=\"FW_PG\", predictionCol=\"prediction\", metricName=\"mse\")\n",
        "mse_test = evaluator_mse.evaluate(predictions_test)\n",
        "print (\"Mean Squared Error (MSE) on test data = %g\" % mse_test)\n",
        "\n",
        "mse_train = evaluator_mse.evaluate(predictions_train)\n",
        "print (\"Mean Squared Error (MSE) on train data = %g\" % mse_train)\n",
        "\n",
        "evaluator_r = RegressionEvaluator(\n",
        "    labelCol=\"FW_PG\", predictionCol=\"prediction\", metricName=\"r2\")\n",
        "r_test = evaluator_r.evaluate(predictions_test)\n",
        "print (\"R2 on test data = %g\" % r_test)\n",
        "\n",
        "r_train = evaluator_r.evaluate(predictions_train)\n",
        "print (\"R2 on train data = %g\" % r_train)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Squared Error (MSE) on test data = 386.858\n",
            "Mean Squared Error (MSE) on train data = 171.669\n",
            "R2 on test data = 0.92872\n",
            "R2 on train data = 0.968462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUKhdhzUO4Ka"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}